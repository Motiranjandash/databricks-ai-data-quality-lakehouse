{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e39ec632-f8e8-4877-b8fb-0282427335d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This section will check the below\n",
    "  - Abnormal daily revenue patterns\n",
    "  - Silent data issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "692a8ee8-9251-47b0-b7c0-1cc0f9e2ef00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# STEP 1: Import required ML library\n",
    "# IsolationForest is used for unsupervised anomaly detection\n",
    "# ------------------------------------------------------------\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 2: Read Gold-level business KPI table from Databricks\n",
    "# This table contains daily aggregated revenue metrics\n",
    "# ------------------------------------------------------------\n",
    "gold_df = spark.table(\n",
    "    \"sales_lakehouse.gold.daily_sales_kpi\"\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 3: Convert Spark DataFrame to Pandas\n",
    "# ML libraries like scikit-learn require Pandas input\n",
    "# Data volume is small here (daily aggregates), so safe to convert\n",
    "# ------------------------------------------------------------\n",
    "pdf = gold_df.toPandas()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 4: Initialize the AI anomaly detection model\n",
    "# contamination = expected percentage of anomalies in data\n",
    "# random_state = ensures reproducible results\n",
    "# ------------------------------------------------------------\n",
    "model = IsolationForest(\n",
    "    contamination=0.025,   # ~3% days expected to be abnormal\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 5: Train the model and detect anomalies\n",
    "# Model learns normal revenue patterns and flags deviations\n",
    "# Output:\n",
    "#   1  = normal data point\n",
    "#  -1  = anomaly\n",
    "# ------------------------------------------------------------\n",
    "pdf[\"revenue_anomaly_flag\"] = model.fit_predict(\n",
    "    pdf[[\"daily_revenue\"]]\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 6: Persist AI quality results back to Databricks\n",
    "# This creates a reusable quality monitoring table\n",
    "# ------------------------------------------------------------\n",
    "quality_df = spark.createDataFrame(pdf)\n",
    "\n",
    "(\n",
    "    quality_df.write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"sales_lakehouse.quality.daily_revenue_anomalies\")\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# STEP 7: View only anomalous records for investigation\n",
    "# These rows i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8940a3-d810-4100-9ba2-4cdaac3c2b08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Revenue Trend with Anomalies (Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf5d91d-d3a0-4eb2-a6c1-8154feffdf0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read quality table\n",
    "pdf = spark.table(\n",
    "    \"sales_lakehouse.quality.daily_revenue_anomalies\"\n",
    ").toPandas()\n",
    "\n",
    "# Split normal and anomaly records\n",
    "normal_df = pdf[pdf[\"revenue_anomaly_flag\"] == 1]\n",
    "anomaly_df = pdf[pdf[\"revenue_anomaly_flag\"] == -1]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot normal revenue trend\n",
    "plt.plot(\n",
    "    normal_df[\"order_date\"],\n",
    "    normal_df[\"daily_revenue\"],\n",
    "    label=\"Normal Revenue\",\n",
    "    linewidth=2\n",
    ")\n",
    "\n",
    "# Highlight anomaly points\n",
    "plt.scatter(\n",
    "    anomaly_df[\"order_date\"],\n",
    "    anomaly_df[\"daily_revenue\"],\n",
    "    label=\"Anomaly\",\n",
    "    s=60\n",
    ")\n",
    "\n",
    "# Chart labels and title\n",
    "plt.title(\"Daily Revenue with AI-Detected Anomalies\")\n",
    "plt.xlabel(\"Order Date\")\n",
    "plt.ylabel(\"Daily Revenue\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d791256f-b79f-42a5-9b9f-ed7b5383197a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Anomaly Summary (KPI View)\n",
    "  \n",
    "  How many days are affected?\n",
    "  How serious is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec556bfb-1254-4222-b480-0b4eec08ad96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import count, when, col\n",
    "\n",
    "summary_df = (\n",
    "    spark.table(\"sales_lakehouse.quality.daily_revenue_anomalies\")\n",
    "    .withColumn(\n",
    "        \"status\",\n",
    "        when(col(\"revenue_anomaly_flag\") == -1, \"Anomaly\")\n",
    "        .otherwise(\"Normal\")\n",
    "    )\n",
    "    .groupBy(\"status\")\n",
    "    .agg(count(\"*\").alias(\"number_of_days\"))\n",
    ")\n",
    "\n",
    "summary_df.display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_ai_data_quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
